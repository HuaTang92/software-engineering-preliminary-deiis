
\section{Adding analysis engines for your pipeline}

You are required to use the type system we included in the archetype to
accomplish this homework. However, if your implementation needs additional data
models (or UIMA types) to store intermediate data, you can also extend the type
system on your own. 
You need to implement an aggregated analysis engine (hw2-ID-aae.xml) with several analysis engines
that annotate the inputs and evaluate the performance of the aggregate analysis engine by
comparing the system outputs with the gold-standard outputs.

We will evaluate your implementation by feeding in unseen inputs and execute your 
\emph{aggregated analysis engine (hw2-ID-aae.xml)}.

You can test your aggregated analysis engine by running the UIMA Document Analyzer 
\url{http://uima.apache.org/d/uimaj-2.4.0/tools.html#ugr.tools.doc_analyzer}. 
We provided an Eclipse run configuration in hw2's archetype run\_configuration\/UIMA Document Analyzer.launch. 

~\\
Here are some suggestions to consider.

\begin{enumerate}

\item If you want to employ a resource (e.g., a model you trained offline) in
your annotator, you could consider UIMA's \emph{resource manager} (refer to the
official tutorial for details about this).
Be sure to put your resource in \texttt{src/main/resources} so that your
submission will also bundle those resources along with your codes.

\item \textbf{If you want to incorporate other NLP or machine learning tools,
please try to avoid non-Java packages. If you want to dependent your artifact on
other Java packages other than those provided by the archetype but can be found
in the course repository, you can add a Maven dependency for this artifact, if
you have the jar package on your machine, but it doesn't exist in the course
repository, please let us know, we will try to deploy them in a 3rd party
repository.}

\item Remember to add comments and Javadocs to your annotators, we will also
evaluate the quality of your codes.

\item The most creative ideas will happen in the intermediate annotators. We
have mentioned some possible solutions to do this. You can try and implement
multiple approaches. All the annotations should be kept in the CAS until a final
``merging'' component takes all the annotations and make a final judgment.

You can mark gene mentions using the type \emph{edu.cmu.deiis.types.Annotation}.
The \emph{casProcessorId} feature 
can be used to indicate a type of the annotator that have tagged a gene name, 
and the \emph{confidence} feature 
can be used to indicate 
an estimated annotation quality (the higher is the confidence value the better
is the annotation).
Note that many statistical NERs produce such confidence values.
For rule-based annotators, you can use some ad hoc fixed value, e.g., one.
You can use these confidence values to aggregate results from several annotators.
For example, you can employ a voting procedure 
in which confidence values are used as voting weights.
If a single annotation is produced by multiple annotators it will have a larger weight
(a sum of individual weights from all contributing annotators)
and a better chance to be selected by the aggregating algorithm (which, e.g.,
can use a threshold to reject low-quality annotations).
This approach was used in IBM Watson system.

\end{enumerate}
